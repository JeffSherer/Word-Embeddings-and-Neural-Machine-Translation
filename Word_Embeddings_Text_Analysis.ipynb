{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O wikitext-filtered-full.zip https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\n",
        "!wget -O wikitext-filtered-10k.zip https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\n",
        "!wget -O visa-outlier-clusters.csv https://www.dropbox.com/scl/fi/duwtwt64uzv504if590sf/visa_outliers_US.csv?rlkey=6nictvov5pjaeue3yatz7m83k&dl=0\n",
        "!wget -O wordsim353.zip https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip"
      ],
      "metadata": {
        "id": "D2ZDySAONXrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip wikitext-filtered-full.zip\n",
        "!unzip wikitext-filtered-10k.zip\n",
        "!unzip wordsim353.zip"
      ],
      "metadata": {
        "id": "-n5Qp5pJNXhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import ast\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from collections.abc import Mapping\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "from scipy import stats\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial import distance\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "from datasets import load_dataset, Dataset\n"
      ],
      "metadata": {
        "id": "FIFRhX8UNXXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "  wikitext_small = \"wikitext-filtered-10k\"\n",
        "  wikitext_large = \"wikitext-filtered-full\"\n",
        "\n",
        "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
        "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
        "  return dataset_small, dataset_large\n",
        "\n",
        "wikitext_small, wikitext_large = load_dataset()\n"
      ],
      "metadata": {
        "id": "5p-9cCXSNXJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d68c00-03f2-41fa-b5f7-8554ab89c7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wikitext_small: 10000 docs, wikitext_large: 859955 docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "\n"
      ],
      "metadata": {
        "id": "FyAe_3GKNurN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a392f807-1020-43a1-f3bc-51b2e28d23b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#First preprocess method that tokenizes the text from the files and then puts them into lowercase\n",
        "# and stores them into a variable if they are not a stopword\n",
        "def Preprocess(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  preprocessed_tokens = [word.lower() for word in tokens if word.lower() not in stopwords_set]\n",
        "  return preprocessed_tokens\n"
      ],
      "metadata": {
        "id": "Z6pIzF21eMby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing the large and small texts\n",
        "preprocessed_wikitext_small = [Preprocess(text) for text in wikitext_small['text']]\n",
        "preprocessed_wikitext_large = [Preprocess(text) for text in wikitext_large['text']]"
      ],
      "metadata": {
        "id": "kzv0_loKeMYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelS = Word2Vec(\n",
        "    vector_size = 50,\n",
        "    sentences = preprocessed_wikitext_small,\n",
        "    window = 5,\n",
        "    epochs = 5,\n",
        "    min_count = 5)"
      ],
      "metadata": {
        "id": "TtaMk-aheMVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelL = Word2Vec(\n",
        "    vector_size = 50,\n",
        "    sentences = preprocessed_wikitext_large,\n",
        "    window = 5,\n",
        "    epochs = 5,\n",
        "    min_count = 5,\n",
        ")"
      ],
      "metadata": {
        "id": "6QNF03nCeMS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#savig and loading the model so that i keep my meausurements consistent\n",
        "modelS.save(\"/content/modelS\")\n",
        "loaded_modelS = Word2Vec.load(\"/content/modelS\")\n",
        "\n",
        "modelL.save(\"/content/modelL\")\n",
        "loaded_modelL = Word2Vec.load(\"/content/modelL\")\n"
      ],
      "metadata": {
        "id": "LbJm-FiAeMQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 Method"
      ],
      "metadata": {
        "id": "_NbTkVz9vH21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read in the csv and store them into combined\n",
        "combined = pd.read_csv(\"combined.csv\")\n",
        "#store the first two columns that contain the words for the pairs into a data frame and then\n",
        "pairs_wiki = combined.iloc[:,0:2]\n",
        "#create tuples of the pairs\n",
        "pairs_wiki = [(word1,word2) for word1,word2 in zip(combined['Word 1'], combined['Word 2'])]"
      ],
      "metadata": {
        "id": "L9Fe8VL9eMJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute cosine similairty for both the large and small models and then\n",
        "def compute_cosine(model, dataframe):\n",
        "\n",
        "  cosine_scores = []\n",
        "  # had issues with using certain features like key_to_index\n",
        "  # checks for the attribute so that those features can be used.\n",
        "  word_vectors = model.wv if hasattr(model,'wv') else model\n",
        "  for index, row in dataframe.iterrows():\n",
        "\n",
        "    word1, word2, human_score = row['Word 1'], row['Word 2'], row['Human (mean)']\n",
        "\n",
        "    #iterating through the words and getting the cosine similarity\n",
        "    if word1 in word_vectors.key_to_index and word2 in word_vectors.key_to_index:\n",
        "      similarity_score = word_vectors.similarity(word1, word2)\n",
        "    else:\n",
        "      #handles oov by setting to 0\n",
        "      similarity_score= 0.0\n",
        "    #adds score to the list\n",
        "    cosine_scores.append(similarity_score)\n",
        "  return cosine_scores\n",
        "#actually computing the scores\n",
        "small_similarity_scores = compute_cosine(modelS, combined)\n",
        "large_similarity_scores = compute_cosine(modelL, combined)\n",
        "#add scores as new column in the csv\n",
        "combined['Similarity (Small)'] = small_similarity_scores\n",
        "combined['Similarity (Large)'] = large_similarity_scores\n",
        "#writing the csv to a new dataframe without the index, index row gave issues when calculating\n",
        "combined.to_csv('combined_no_index.csv', index = False)\n"
      ],
      "metadata": {
        "id": "1wHl2anceMBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 Answers"
      ],
      "metadata": {
        "id": "zKXXt-jNvAHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_pairs = [\n",
        "    ('plane', 'car'),\n",
        "    ('planet', 'sun'),\n",
        "    ('cup', 'article'),\n",
        "    ('sugar', 'approach')\n",
        "]\n",
        "\n",
        "#iterate through list by finding the word pair from the examples in the combined and getting the scores for the large and small model\n",
        "for word1, word2 in example_pairs:\n",
        "  pair_row = combined[(combined['Word 1'] == word1) & (combined['Word 2'] == word2)]\n",
        "  small_score = pair_row['Similarity (Small)'].values[0]\n",
        "  large_score = pair_row['Similarity (Large)'].values[0]\n",
        "\n",
        "  print(f\"Cosine Similarity using the Small Model for {word1} and {word2} is {small_score:.4f}\")\n",
        "  print(f\"Cosine Similarity using the Large Model for {word1} and {word2} is {large_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "cbJQXxgveL85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15483427-f025-4376-c66e-b560ea09f6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity using the Small Model for plane and car is 0.9207\n",
            "Cosine Similarity using the Large Model for plane and car is 0.6393\n",
            "Cosine Similarity using the Small Model for planet and sun is 0.9564\n",
            "Cosine Similarity using the Large Model for planet and sun is 0.6501\n",
            "Cosine Similarity using the Small Model for cup and article is 0.4876\n",
            "Cosine Similarity using the Large Model for cup and article is 0.0783\n",
            "Cosine Similarity using the Small Model for sugar and approach is 0.9369\n",
            "Cosine Similarity using the Large Model for sugar and approach is -0.2205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 Method"
      ],
      "metadata": {
        "id": "q-aYn0NVvNxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get the similarity scores and humans mean to compute spearmans\n",
        "small_cosine_scores = combined[\"Similarity (Small)\"]\n",
        "large_cosine_scores = combined[\"Similarity (Large)\"]\n",
        "human_scores = combined[\"Human (mean)\"]\n",
        "\n",
        "#compute the spearman and p value, i only display spearmans though\n",
        "spearman_small, p_value_s = stats.spearmanr(small_cosine_scores, human_scores)\n",
        "spearman_large, p_value_l = stats.spearmanr(large_cosine_scores, human_scores)"
      ],
      "metadata": {
        "id": "FqoT68sMSw5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step Answers"
      ],
      "metadata": {
        "id": "4OMqn7tIvYTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Spearman (small model) is {spearman_small:.4f}\")\n",
        "print(f\"Spearman (large model) is {spearman_large:.4f}\")"
      ],
      "metadata": {
        "id": "xNQ4ogSRvXgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec9b96c-a55f-4cbd-cf8a-39f83fbe76b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman (small model) is -0.0704\n",
            "Spearman (large model) is 0.5315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 Method"
      ],
      "metadata": {
        "id": "m0fsCervXy75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# different preprocess where i tokenize differently, i was having issues with the other preprocess method for this file\n",
        "# so did a lot of trouble shooting where i tokenized differently by converting the columns to lists and then preproces\n",
        "def Preprocess2(concept_list):\n",
        "  return [concept.lower() for concept in concept_list if concept.lower() not in stopwords_set]\n"
      ],
      "metadata": {
        "id": "5pcKx3Dldj9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outlier_clusters_df = pd.read_csv('/content/visa-outlier-clusters.csv')\n",
        "\n",
        "# where i turn the columns into lists and preprocess the relevant columnsoutlier_clusters_df['concepts'] = outlier_clusters_df['concepts'].apply(ast.literal_eval).apply(Preprocess2)\n",
        "outlier_clusters_df['concepts'] = outlier_clusters_df['concepts'].apply(ast.literal_eval).apply(Preprocess2)\n",
        "outlier_clusters_df['labels'] = outlier_clusters_df['labels'].apply(ast.literal_eval)\n",
        "\n",
        "# general compactness method that works for both models and exlcudes the word of interest in the cluster\n",
        "# so that it does not computer the simialirty score against itself\n",
        "#returns summation of simlairty scores and then normalizes it\n",
        "def compactness_score_without_word(cluster, word_to_exclude, model):\n",
        "    #create a new list without deleted word\n",
        "    cluster_without_word = [word for word in cluster if word != word_to_exclude]\n",
        "    total_sim = 0\n",
        "    #same check for wv in order to use the key_to_index function\n",
        "    word_vectors = model.wv if hasattr(model,'wv') else model\n",
        "    #calculating the simialirty for each word pair in the new cluster\n",
        "    for i in range(len(cluster_without_word)):\n",
        "        for j in range(len(cluster_without_word)):\n",
        "            word1 = cluster_without_word[i]\n",
        "            word2 = cluster_without_word[j]\n",
        "\n",
        "            if i != j and word1 in word_vectors.key_to_index and word2 in word_vectors.key_to_index:\n",
        "                similarity = word_vectors.similarity(word1, word2)\n",
        "                total_sim += similarity\n",
        "\n",
        "    return total_sim / (len(cluster_without_word) * (len(cluster_without_word) - 1))\n",
        "# intialize lists so that i can calculate the accuracy efficiently\n",
        "true_labels_small = []\n",
        "predicted_outlier_indices_small = []\n",
        "true_labels_large = []\n",
        "predicted_outlier_indices_large = []\n",
        "\n",
        "# iteration through each cluster in the dataset and\n",
        "for index, row in outlier_clusters_df.iterrows():\n",
        "\n",
        "    #gettng columns as lists\n",
        "    concept_list = row['concepts']\n",
        "    binary_labels = row['labels']\n",
        "\n",
        "    #merging the two variables. I realie that i could condense as they are the same thing, but i did not have enough time\n",
        "    true_labels_small.append(binary_labels.index(1))\n",
        "    true_labels_large.append(binary_labels.index(1))\n",
        "\n",
        "    #initilaizing the variables so that I can store the indices and highest score of predicted outliers\n",
        "    highest_score_small = -1\n",
        "    outlier_index_small = -1\n",
        "    highest_score_large = -1\n",
        "    outlier_index_large = -1\n",
        "\n",
        "    #iterate through each concept and calcuates compactness score\n",
        "    for i, concept in enumerate(concept_list):\n",
        "        compactness_score_small = compactness_score_without_word(concept_list, concept, modelS)\n",
        "        compactness_score_large = compactness_score_without_word(concept_list, concept, modelL)\n",
        "        #updates the highest scores and indices of those scores for predicting outliers\n",
        "        if compactness_score_small > highest_score_small:\n",
        "            highest_score_small = compactness_score_small\n",
        "            outlier_index_small = i\n",
        "        if compactness_score_large > highest_score_large:\n",
        "            highest_score_large = compactness_score_large\n",
        "            outlier_index_large = i\n",
        "\n",
        "    #updates the list with all of the indices of the predicted outliers accoridng to each model\n",
        "    predicted_outlier_indices_small.append(outlier_index_small)\n",
        "    predicted_outlier_indices_large.append(outlier_index_large)\n",
        "\n",
        "#compares true outliers with the predicted outliers according to their index, and computes accuracy\n",
        "accuracy_small = np.mean(np.array(true_labels_small) == np.array(predicted_outlier_indices_small))\n",
        "accuracy_large = np.mean(np.array(true_labels_large) == np.array(predicted_outlier_indices_large))\n"
      ],
      "metadata": {
        "id": "4oRco4Z3dieb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 Answers"
      ],
      "metadata": {
        "id": "KnFsO7GTvdw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy Score (Small Model) for Outlier Detection Task: {accuracy_small:.4f}\")\n",
        "print(f\"Accuracy Score (Large Model) for Outlier Detection Task: {accuracy_large:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqpo57y-vh5T",
        "outputId": "c7c64f33-64bc-48b4-c8ad-49604cd07371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score (Small Model) for Outlier Detection Task: 0.1000\n",
            "Accuracy Score (Large Model) for Outlier Detection Task: 0.4250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6 Method"
      ],
      "metadata": {
        "id": "egiovYPMjNUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelG = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "0DSeVAk-chVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes similarity scores using the google news model\n",
        "modelG_similarity_scores = compute_cosine(modelG, combined)\n",
        "\n",
        "# adds a new column to combined\n",
        "combined['Similarity (ModelG)'] = modelG_similarity_scores\n",
        "human_scores = combined[\"Human (mean)\"]\n",
        "\n",
        "spearman_modelG, p_value_G = stats.spearmanr(modelG_similarity_scores, human_scores)\n"
      ],
      "metadata": {
        "id": "Xvl7cU1tsbfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#big trend with this block of code is that when i started to seperate or use other function previously\n",
        "# defined, i ran into issues. So i redefined a lot of functions, in order to ensure that it works. ideally i\n",
        "# would not do this but do not have the time to figure out where I am going wrong\n",
        "def Preprocess2(concept_list):\n",
        "\n",
        "    return [concept.lower() for concept in concept_list if concept.lower() not in stopwords_set]\n",
        "\n",
        "# i have dowloaded this twice, I am aware that I shouldnt have to do this. for some reason my code\n",
        "# doesnt like it if i dont include it here. often gives me the wrong results\n",
        "outlier_clusters_df = pd.read_csv('/content/visa-outlier-clusters.csv')\n",
        "\n",
        "# converting to lists again\n",
        "outlier_clusters_df['concepts'] = outlier_clusters_df['concepts'].apply(ast.literal_eval).apply(Preprocess2)\n",
        "outlier_clusters_df['labels'] = outlier_clusters_df['labels'].apply(ast.literal_eval)\n",
        "\n",
        "#same method as above, i am not sure what I am doing wrong but i needed to define it twice for it to function.\n",
        "#it is functionally the same, but small differences like how i store \"n\". Ideally I would make the one function\n",
        "#defined above work here. If i had more time that would be part of what I would fix\n",
        "def compactness_score_without_word(cluster, word_to_exclude, model):\n",
        "    cluster_without_word = [word for word in cluster if word != word_to_exclude]\n",
        "    total_sim = 0\n",
        "\n",
        "    for i in range(len(cluster_without_word)):\n",
        "        for j in range(len(cluster_without_word)):\n",
        "            word1 = cluster_without_word[i]\n",
        "            word2 = cluster_without_word[j]\n",
        "\n",
        "            if i != j and word1 in model.key_to_index and word2 in model.key_to_index:\n",
        "                similarity = model.similarity(word1, word2)\n",
        "                total_sim += similarity\n",
        "\n",
        "    n = len(cluster_without_word)\n",
        "    return total_sim / (n * (n - 1)) if n > 1 else 0.0\n",
        "\n",
        "true_labels_extra_large = []\n",
        "predicted_outlier_indices_extra_large = []\n",
        "\n",
        "# Loop through the clusters and determine the highest compactness score and indices\n",
        "for index, row in outlier_clusters_df.iterrows():\n",
        "    concept_list = row['concepts']\n",
        "    binary_labels = row['labels']\n",
        "\n",
        "    true_labels_extra_large.append(binary_labels.index(1))\n",
        "\n",
        "    highest_score_extra_large = -1\n",
        "    outlier_index_extra_large = -1\n",
        "\n",
        "    for i, concept in enumerate(concept_list):\n",
        "        compactness_score_extra_large = compactness_score_without_word(concept_list, concept, modelG)\n",
        "\n",
        "        if compactness_score_extra_large > highest_score_extra_large:\n",
        "            highest_score_extra_large = compactness_score_extra_large\n",
        "            outlier_index_extra_large = i\n",
        "\n",
        "    predicted_outlier_indices_extra_large.append(outlier_index_extra_large)\n",
        "\n",
        "accuracy_extra_large = np.mean(np.array(true_labels_extra_large) == np.array(predicted_outlier_indices_extra_large))\n",
        "\n"
      ],
      "metadata": {
        "id": "LmrB47AEcwEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6 Answers"
      ],
      "metadata": {
        "id": "hO4mIYRrvttf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Spearman (modelG) is {spearman_modelG:.4f}\")\n",
        "\n",
        "print(f\"Accuracy for the Extra Large Model: {accuracy_extra_large:.4f}\")"
      ],
      "metadata": {
        "id": "ESfTaM8hvsMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d3888c-00e8-4b37-f8b9-8debdd453ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman (modelG) is 0.7000\n",
            "Accuracy for the Extra Large Model: 0.4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 8"
      ],
      "metadata": {
        "id": "ckltycv6wm8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function that calculate the analogy by using the most_similar method between word 3 and word 1\n",
        "# and it is not similar to word 2.it looks to capture the relationship between word 1 and 2,\n",
        "#as with word 3 and the analogies. It filters out word 1 and word 3 results.\n",
        "def find_analogy(word1, word2, word3, wv, topn=5):\n",
        "    analogy = wv.most_similar(positive = [word3, word1], negative = [word2], topn=topn)\n",
        "    words = [word for word, score in analogy if word!= word1 and word != word3]\n",
        "    return words"
      ],
      "metadata": {
        "id": "k7oCxNGfcv52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogies_examples = [\n",
        "    (\"man\", \"woman\", \"king\"),\n",
        "    (\"Athens\", \"Greece\", \"Rome\"),\n",
        "    (\"reading\", \"read\", \"playing\"),\n",
        "    (\"Greece\", \"souvlaki\", \"Italy\"),\n",
        "    (\"airplane\", \"propeller\", \"car\"),\n",
        "    (\"man\", \"woman\", \"computer_programmer\"),\n",
        "    (\"man\", \"woman\", \"superstar\"),\n",
        "    (\"man\", \"woman\", \"guitarist\"),\n",
        "    (\"man\", \"woman\", \"boss\"),\n",
        "]\n",
        "#dictionary store the results\n",
        "results = {}\n",
        "#loops through each set of words and find the analogy, stores it into the dictionary\n",
        "for w1, w2, w3 in analogies_examples:\n",
        "  results[f\"{w1} is to {w2} as {w3} is to\"] = find_analogy(w1,w2,w3,modelG)"
      ],
      "metadata": {
        "id": "Y5PKBZNmcv2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prints the answer for all analogies in the results\n",
        "for analogy, answer in results.items():\n",
        "    print(f\"{analogy} {answer}\")"
      ],
      "metadata": {
        "id": "er7RqI6Lcvvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}